{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joshhu/llmdistill/blob/main/llm_distillation_demo.ipynb)\n\n# LLM Knowledge Distillation ç¤ºç¯„\n\né€™å€‹ notebook ç¤ºç¯„å¦‚ä½•ä½¿ç”¨çŸ¥è­˜è’¸é¤¾ï¼ˆKnowledge Distillationï¼‰ä¾†è¨“ç·´èªè¨€æ¨¡å‹ã€‚\n\n## æµç¨‹èªªæ˜\n1. **è€å¸«æ¨¡å‹ï¼ˆTeacherï¼‰**: GPT-2 (117M åƒæ•¸) - ç”¨ä¾†ç”Ÿæˆè¨“ç·´è³‡æ–™\n2. **å­¸ç”Ÿæ¨¡å‹ï¼ˆStudentï¼‰**: è¼ƒå°çš„ GPT-2 é…ç½® (ç´„ 40M åƒæ•¸) - å¾è€å¸«ç”Ÿæˆçš„è³‡æ–™å­¸ç¿’\n3. **è©•ä¼°æŒ‡æ¨™**: ä½¿ç”¨ Perplexity ä¾†è¡¡é‡æ¨¡å‹æ•ˆæœ\n\n## ç¡¬é«”éœ€æ±‚\n- GPU: 16GB VRAM (Colab Free Tier T4 å¯åŸ·è¡Œ)\n- è¨“ç·´æ™‚é–“: ç´„ 15-20 åˆ†é˜"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "!pip install -q transformers datasets torch accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æª¢æŸ¥ GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è£ç½®: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU åç¨±: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: è¼‰å…¥è€å¸«æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è€å¸«æ¨¡å‹ (GPT-2)\n",
    "print(\"è¼‰å…¥è€å¸«æ¨¡å‹ (GPT-2)...\")\n",
    "teacher_model_name = \"gpt2\"\n",
    "teacher_model = GPT2LMHeadModel.from_pretrained(teacher_model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(teacher_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# çµ±è¨ˆè€å¸«æ¨¡å‹åƒæ•¸é‡\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"è€å¸«æ¨¡å‹åƒæ•¸é‡: {teacher_params:,} ({teacher_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: ä½¿ç”¨è€å¸«æ¨¡å‹ç”Ÿæˆè¨“ç·´è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_training_data(model, tokenizer, prompts, num_samples=100, max_length=128):\n    model.eval()\n    generated_texts = []\n    \n    print(f\"\\nç”Ÿæˆ {num_samples} å€‹è¨“ç·´æ¨£æœ¬...\")\n    \n    with torch.no_grad():\n        for i in tqdm(range(num_samples)):\n            prompt = prompts[i % len(prompts)]\n            \n            # ä½¿ç”¨ tokenizer() è€Œé encode() ä»¥ç²å¾— attention_mask\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            input_ids = inputs['input_ids']\n            attention_mask = inputs['attention_mask']\n            \n            output = model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_return_sequences=1,\n                temperature=0.9,\n                top_k=50,\n                top_p=0.95,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n            \n            text = tokenizer.decode(output[0], skip_special_tokens=True)\n            generated_texts.append(text)\n    \n    return generated_texts\n\n# å®šç¾©å¤šæ¨£åŒ–çš„æç¤ºè©\nprompts = [\n    \"Once upon a time\",\n    \"The future of technology\",\n    \"In a world where\",\n    \"Scientists have discovered\",\n    \"The story begins with\",\n    \"Artificial intelligence is\",\n    \"Climate change has\",\n    \"In the year 2050\",\n    \"The most important thing\",\n    \"Machine learning can\"\n]\n\n# ç”Ÿæˆè¨“ç·´è³‡æ–™\nnum_training_samples = 500\ngenerated_texts = generate_training_data(\n    teacher_model, \n    tokenizer, \n    prompts, \n    num_samples=num_training_samples,\n    max_length=100\n)\n\nprint(f\"\\nç”Ÿæˆå®Œæˆ! å…± {len(generated_texts)} å€‹æ¨£æœ¬\")\nprint(\"\\nç¯„ä¾‹ç”Ÿæˆæ–‡æœ¬:\")\nfor i in range(3):\n    print(f\"\\n[{i+1}] {generated_texts[i][:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: å»ºç«‹å­¸ç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¼ƒå°çš„å­¸ç”Ÿæ¨¡å‹é…ç½®\n",
    "print(\"\\nå»ºç«‹å­¸ç”Ÿæ¨¡å‹...\")\n",
    "student_config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=1024,\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    n_inner=2048,\n",
    ")\n",
    "\n",
    "student_model = GPT2LMHeadModel(student_config).to(device)\n",
    "\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"å­¸ç”Ÿæ¨¡å‹åƒæ•¸é‡: {student_params:,} ({student_params/1e6:.1f}M)\")\n",
    "print(f\"åƒæ•¸å£“ç¸®æ¯”: {teacher_params/student_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: æº–å‚™è¨“ç·´è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(texts, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    dataset_dict = {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask']\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    return dataset\n",
    "\n",
    "print(\"\\næº–å‚™è¨“ç·´è³‡æ–™é›†...\")\n",
    "split_idx = int(len(generated_texts) * 0.9)\n",
    "train_texts = generated_texts[:split_idx]\n",
    "val_texts = generated_texts[split_idx:]\n",
    "\n",
    "train_dataset = prepare_dataset(train_texts, tokenizer, max_length=100)\n",
    "val_dataset = prepare_dataset(val_texts, tokenizer, max_length=100)\n",
    "\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"é©—è­‰é›†å¤§å°: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: è©•ä¼°å‡½æ•¸ - è¨ˆç®— Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, dataset, tokenizer, batch_size=8):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"è¨ˆç®— Perplexity\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            total_tokens += input_ids.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "print(\"\\nè©•ä¼°è¨“ç·´å‰çš„å­¸ç”Ÿæ¨¡å‹...\")\n",
    "initial_perplexity = calculate_perplexity(student_model, val_dataset, tokenizer)\n",
    "print(f\"è¨“ç·´å‰ Perplexity: {initial_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6: è¨“ç·´å­¸ç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./student_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\né–‹å§‹è¨“ç·´å­¸ç”Ÿæ¨¡å‹...\")\n",
    "print(\"=\" * 50)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 7: è©•ä¼°è¨“ç·´å¾Œçš„å­¸ç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nè©•ä¼°è¨“ç·´å¾Œçš„å­¸ç”Ÿæ¨¡å‹...\")\n",
    "final_perplexity = calculate_perplexity(student_model, val_dataset, tokenizer)\n",
    "print(f\"è¨“ç·´å¾Œ Perplexity: {final_perplexity:.2f}\")\n",
    "\n",
    "improvement = (initial_perplexity - final_perplexity) / initial_perplexity * 100\n",
    "print(f\"\\næ”¹å–„ç‡: {improvement:.2f}%\")\n",
    "\n",
    "print(\"\\nè©•ä¼°è€å¸«æ¨¡å‹ (ä½œç‚ºå°ç…§)...\")\n",
    "teacher_perplexity = calculate_perplexity(teacher_model, val_dataset, tokenizer)\n",
    "print(f\"è€å¸«æ¨¡å‹ Perplexity: {teacher_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 8: è¦–è¦ºåŒ–çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['å­¸ç”Ÿæ¨¡å‹\\n(è¨“ç·´å‰)', 'å­¸ç”Ÿæ¨¡å‹\\n(è¨“ç·´å¾Œ)', 'è€å¸«æ¨¡å‹']\n",
    "perplexities = [initial_perplexity, final_perplexity, teacher_perplexity]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, perplexities, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.ylabel('Perplexity (è¶Šä½è¶Šå¥½)', fontsize=12, fontweight='bold')\n",
    "plt.title('Knowledge Distillation æ•ˆæœæ¯”è¼ƒ', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, ppl in zip(bars, perplexities):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{ppl:.2f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distillation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nåœ–è¡¨å·²å„²å­˜ç‚º 'distillation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 9: ç”Ÿæˆæ–‡æœ¬æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_generation(teacher, student, tokenizer, prompt, max_length=100):\n    teacher.eval()\n    student.eval()\n    \n    # ä½¿ç”¨ tokenizer() è€Œé encode() ä»¥ç²å¾— attention_mask\n    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    \n    with torch.no_grad():\n        teacher_output = teacher.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=max_length,\n            temperature=0.8,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        teacher_text = tokenizer.decode(teacher_output[0], skip_special_tokens=True)\n        \n        student_output = student.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=max_length,\n            temperature=0.8,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        student_text = tokenizer.decode(student_output[0], skip_special_tokens=True)\n    \n    return teacher_text, student_text\n\ntest_prompts = [\n    \"The future of artificial intelligence\",\n    \"In a distant galaxy\",\n    \"Climate change is\"\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"æ–‡æœ¬ç”Ÿæˆæ¯”è¼ƒ\")\nprint(\"=\"*60)\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\nã€æ¸¬è©¦ {i}ã€‘æç¤ºè©: \\\"{prompt}\\\"\")\n    print(\"-\" * 60)\n    \n    teacher_text, student_text = compare_generation(\n        teacher_model, student_model, tokenizer, prompt, max_length=80\n    )\n    \n    print(f\"\\nğŸ“ è€å¸«æ¨¡å‹:\\n{teacher_text}\")\n    print(f\"\\nğŸ’ å­¸ç”Ÿæ¨¡å‹:\\n{student_text}\")\n    print(\"-\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµå ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Knowledge Distillation å¯¦é©—ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹è¦æ ¼:\")\n",
    "print(f\"  â€¢ è€å¸«æ¨¡å‹: GPT-2 ({teacher_params/1e6:.1f}M åƒæ•¸)\")\n",
    "print(f\"  â€¢ å­¸ç”Ÿæ¨¡å‹: GPT-2-Small ({student_params/1e6:.1f}M åƒæ•¸)\")\n",
    "print(f\"  â€¢ åƒæ•¸å£“ç¸®æ¯”: {teacher_params/student_params:.2f}x\")\n",
    "\n",
    "print(f\"\\nğŸ“š è¨“ç·´è³‡æ–™:\")\n",
    "print(f\"  â€¢ è¨“ç·´æ¨£æœ¬æ•¸: {len(train_texts)}\")\n",
    "print(f\"  â€¢ é©—è­‰æ¨£æœ¬æ•¸: {len(val_texts)}\")\n",
    "print(f\"  â€¢ è³‡æ–™ä¾†æº: è€å¸«æ¨¡å‹ç”Ÿæˆ\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Perplexity çµæœ:\")\n",
    "print(f\"  â€¢ è€å¸«æ¨¡å‹: {teacher_perplexity:.2f}\")\n",
    "print(f\"  â€¢ å­¸ç”Ÿæ¨¡å‹ (è¨“ç·´å‰): {initial_perplexity:.2f}\")\n",
    "print(f\"  â€¢ å­¸ç”Ÿæ¨¡å‹ (è¨“ç·´å¾Œ): {final_perplexity:.2f}\")\n",
    "print(f\"  â€¢ æ”¹å–„ç‡: {improvement:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ¯ é”æˆç‡åˆ†æ:\")\n",
    "achievement_rate = (teacher_perplexity / final_perplexity) * 100\n",
    "print(f\"  â€¢ å­¸ç”Ÿæ¨¡å‹é”æˆç‡: {achievement_rate:.2f}%\")\n",
    "print(f\"    (ç›¸å°æ–¼è€å¸«æ¨¡å‹çš„ perplexity è¡¨ç¾)\")\n",
    "\n",
    "efficiency_gain = (teacher_params / student_params) / (final_perplexity / teacher_perplexity)\n",
    "print(f\"  â€¢ æ•ˆç‡æå‡: {efficiency_gain:.2f}x\")\n",
    "print(f\"    (åƒæ•¸æ¸›å°‘ vs. æ•ˆèƒ½æå¤±æ¯”)\")\n",
    "\n",
    "print(f\"\\nâœ… çµè«–:\")\n",
    "print(f\"  å­¸ç”Ÿæ¨¡å‹æˆåŠŸå¾è€å¸«æ¨¡å‹ç”Ÿæˆçš„è³‡æ–™ä¸­å­¸ç¿’!\")\n",
    "print(f\"  ä½¿ç”¨åƒ… {student_params/teacher_params*100:.1f}% çš„åƒæ•¸é‡,\")\n",
    "print(f\"  é”åˆ°è€å¸«æ¨¡å‹ {achievement_rate:.1f}% çš„æ•ˆèƒ½ã€‚\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}