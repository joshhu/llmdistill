{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joshhu/llmdistill/blob/main/llm_distillation_demo.ipynb)\n\n# LLM Knowledge Distillation ç¤ºç¯„\n\né€™å€‹ notebook ç¤ºç¯„å¦‚ä½•ä½¿ç”¨çŸ¥è­˜è’¸é¤¾ï¼ˆKnowledge Distillationï¼‰ä¾†è¨“ç·´èªè¨€æ¨¡å‹ã€‚\n\n## æµç¨‹èªªæ˜\n1. **è€å¸«æ¨¡å‹ï¼ˆTeacherï¼‰**: GPT-2 (117M åƒæ•¸) - ç”¨ä¾†ç”Ÿæˆè¨“ç·´è³‡æ–™\n2. **å­¸ç”Ÿæ¨¡å‹ï¼ˆStudentï¼‰**: è¼ƒå°çš„ GPT-2 é…ç½® (ç´„ 40M åƒæ•¸) - å¾è€å¸«ç”Ÿæˆçš„è³‡æ–™å­¸ç¿’\n3. **è©•ä¼°æŒ‡æ¨™**: ä½¿ç”¨ Perplexity ä¾†è¡¡é‡æ¨¡å‹æ•ˆæœ\n\n## ç¡¬é«”éœ€æ±‚\n- GPU: 16GB VRAM (Colab Free Tier T4 å¯åŸ·è¡Œ)\n- è¨“ç·´æ™‚é–“: ç´„ 15-20 åˆ†é˜"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å®‰è£å¿…è¦å¥—ä»¶\n!pip install -q transformers datasets torch accelerate sentencepiece\n\n# å®‰è£ä¸­æ–‡å­—é«”æ”¯æ´ï¼ˆåœ¨ Colab ç’°å¢ƒï¼‰\n!apt-get install -qq fonts-noto-cjk\nimport matplotlib.font_manager as fm\nfm._rebuild()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom transformers import (\n    GPT2LMHeadModel, \n    GPT2Tokenizer, \n    GPT2Config,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# è¨­å®š matplotlib ä¸­æ–‡å­—é«”æ”¯æ´\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei', 'Microsoft YaHei']\nplt.rcParams['axes.unicode_minus'] = False  # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ\n\n# æª¢æŸ¥ GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ä½¿ç”¨è£ç½®: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU åç¨±: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: è¼‰å…¥è€å¸«æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è€å¸«æ¨¡å‹ (GPT-2)\n",
    "print(\"è¼‰å…¥è€å¸«æ¨¡å‹ (GPT-2)...\")\n",
    "teacher_model_name = \"gpt2\"\n",
    "teacher_model = GPT2LMHeadModel.from_pretrained(teacher_model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(teacher_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# çµ±è¨ˆè€å¸«æ¨¡å‹åƒæ•¸é‡\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"è€å¸«æ¨¡å‹åƒæ•¸é‡: {teacher_params:,} ({teacher_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: ä½¿ç”¨è€å¸«æ¨¡å‹ç”Ÿæˆè¨“ç·´è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_training_data(model, tokenizer, prompts, num_samples=100, max_length=128):\n    model.eval()\n    generated_texts = []\n    \n    print(f\"\\nç”Ÿæˆ {num_samples} å€‹è¨“ç·´æ¨£æœ¬...\")\n    \n    with torch.no_grad():\n        for i in tqdm(range(num_samples)):\n            prompt = prompts[i % len(prompts)]\n            \n            # ä½¿ç”¨ tokenizer() è€Œé encode() ä»¥ç²å¾— attention_mask\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            input_ids = inputs['input_ids']\n            attention_mask = inputs['attention_mask']\n            \n            output = model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_return_sequences=1,\n                temperature=0.9,\n                top_k=50,\n                top_p=0.95,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n            \n            text = tokenizer.decode(output[0], skip_special_tokens=True)\n            generated_texts.append(text)\n    \n    return generated_texts\n\n# å®šç¾©å¤šæ¨£åŒ–çš„æç¤ºè©\nprompts = [\n    \"Once upon a time\",\n    \"The future of technology\",\n    \"In a world where\",\n    \"Scientists have discovered\",\n    \"The story begins with\",\n    \"Artificial intelligence is\",\n    \"Climate change has\",\n    \"In the year 2050\",\n    \"The most important thing\",\n    \"Machine learning can\"\n]\n\n# ç”Ÿæˆè¨“ç·´è³‡æ–™\nnum_training_samples = 500\ngenerated_texts = generate_training_data(\n    teacher_model, \n    tokenizer, \n    prompts, \n    num_samples=num_training_samples,\n    max_length=100\n)\n\nprint(f\"\\nç”Ÿæˆå®Œæˆ! å…± {len(generated_texts)} å€‹æ¨£æœ¬\")\nprint(\"\\nç¯„ä¾‹ç”Ÿæˆæ–‡æœ¬:\")\nfor i in range(3):\n    print(f\"\\n[{i+1}] {generated_texts[i][:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: å»ºç«‹å­¸ç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¼ƒå°çš„å­¸ç”Ÿæ¨¡å‹é…ç½®\n",
    "print(\"\\nå»ºç«‹å­¸ç”Ÿæ¨¡å‹...\")\n",
    "student_config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=1024,\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    n_inner=2048,\n",
    ")\n",
    "\n",
    "student_model = GPT2LMHeadModel(student_config).to(device)\n",
    "\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"å­¸ç”Ÿæ¨¡å‹åƒæ•¸é‡: {student_params:,} ({student_params/1e6:.1f}M)\")\n",
    "print(f\"åƒæ•¸å£“ç¸®æ¯”: {teacher_params/student_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: æº–å‚™è¨“ç·´è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_dataset(texts, tokenizer, max_length=128):\n    encodings = tokenizer(\n        texts, \n        truncation=True, \n        padding='max_length',\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    # ç¢ºä¿è¿”å›æ­£ç¢ºçš„å¼µé‡æ ¼å¼\n    dataset_dict = {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask']\n    }\n    \n    # è¨­ç½®æ ¼å¼ç‚º torch\n    dataset = Dataset.from_dict(dataset_dict)\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    \n    return dataset\n\nprint(\"\\næº–å‚™è¨“ç·´è³‡æ–™é›†...\")\nsplit_idx = int(len(generated_texts) * 0.9)\ntrain_texts = generated_texts[:split_idx]\nval_texts = generated_texts[split_idx:]\n\ntrain_dataset = prepare_dataset(train_texts, tokenizer, max_length=100)\nval_dataset = prepare_dataset(val_texts, tokenizer, max_length=100)\n\nprint(f\"è¨“ç·´é›†å¤§å°: {len(train_dataset)}\")\nprint(f\"é©—è­‰é›†å¤§å°: {len(val_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: è©•ä¼°å‡½æ•¸ - è¨ˆç®— Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_perplexity(model, dataset, tokenizer, batch_size=8):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    from torch.utils.data import DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"è¨ˆç®— Perplexity\"):\n            # ç¢ºä¿æ•¸æ“šæ˜¯ tensor æ ¼å¼ä¸¦ç§»åˆ°æ­£ç¢ºçš„è¨­å‚™\n            if isinstance(batch['input_ids'], list):\n                input_ids = torch.tensor(batch['input_ids']).to(device)\n                attention_mask = torch.tensor(batch['attention_mask']).to(device)\n            else:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=input_ids\n            )\n            \n            loss = outputs.loss\n            total_loss += loss.item() * input_ids.size(0)\n            total_tokens += input_ids.size(0)\n    \n    avg_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(avg_loss))\n    \n    return perplexity.item()\n\nprint(\"\\nè©•ä¼°è¨“ç·´å‰çš„å­¸ç”Ÿæ¨¡å‹...\")\ninitial_perplexity = calculate_perplexity(student_model, val_dataset, tokenizer)\nprint(f\"è¨“ç·´å‰ Perplexity: {initial_perplexity:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6: è¨“ç·´å­¸ç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./student_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\né–‹å§‹è¨“ç·´å­¸ç”Ÿæ¨¡å‹...\")\n",
    "print(\"=\" * 50)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 7: è©•ä¼°è¨“ç·´å¾Œçš„å­¸ç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nè©•ä¼°è¨“ç·´å¾Œçš„å­¸ç”Ÿæ¨¡å‹...\")\n",
    "final_perplexity = calculate_perplexity(student_model, val_dataset, tokenizer)\n",
    "print(f\"è¨“ç·´å¾Œ Perplexity: {final_perplexity:.2f}\")\n",
    "\n",
    "improvement = (initial_perplexity - final_perplexity) / initial_perplexity * 100\n",
    "print(f\"\\næ”¹å–„ç‡: {improvement:.2f}%\")\n",
    "\n",
    "print(\"\\nè©•ä¼°è€å¸«æ¨¡å‹ (ä½œç‚ºå°ç…§)...\")\n",
    "teacher_perplexity = calculate_perplexity(teacher_model, val_dataset, tokenizer)\n",
    "print(f\"è€å¸«æ¨¡å‹ Perplexity: {teacher_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## æ­¥é©Ÿ 7.5: è¨ˆç®— KL Divergenceï¼ˆåˆ†å¸ƒç›¸ä¼¼åº¦ï¼‰\n\nKL Divergence è¡¡é‡å­¸ç”Ÿæ¨¡å‹å’Œè€å¸«æ¨¡å‹è¼¸å‡ºåˆ†å¸ƒçš„ç›¸ä¼¼åº¦ï¼Œæ˜¯è©•ä¼°çŸ¥è­˜è’¸é¤¾æ•ˆæœçš„é‡è¦æŒ‡æ¨™ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_kl_divergence(teacher, student, dataset, tokenizer, batch_size=8, num_samples=50):\n    \"\"\"\n    è¨ˆç®—å­¸ç”Ÿæ¨¡å‹å’Œè€å¸«æ¨¡å‹è¼¸å‡ºåˆ†å¸ƒçš„ KL Divergence\n    \n    Args:\n        teacher: è€å¸«æ¨¡å‹\n        student: å­¸ç”Ÿæ¨¡å‹\n        dataset: è©•ä¼°è³‡æ–™é›†\n        tokenizer: åˆ†è©å™¨\n        batch_size: æ‰¹æ¬¡å¤§å°\n        num_samples: è¨ˆç®—çš„æ¨£æœ¬æ•¸é‡ï¼ˆç‚ºäº†æ•ˆç‡ï¼‰\n    \n    Returns:\n        å¹³å‡ KL divergence å€¼\n    \"\"\"\n    import torch.nn.functional as F\n    \n    teacher.eval()\n    student.eval()\n    \n    total_kl = 0\n    count = 0\n    \n    from torch.utils.data import DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n    \n    print(f\"\\nè¨ˆç®— KL Divergenceï¼ˆä½¿ç”¨ {num_samples} å€‹æ¨£æœ¬ï¼‰...\")\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"è¨ˆç®— KL Divergence\"):\n            if count >= num_samples:\n                break\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            # ç²å–è€å¸«å’Œå­¸ç”Ÿçš„ logits\n            teacher_outputs = teacher(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            student_outputs = student(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            teacher_logits = teacher_outputs.logits\n            student_logits = student_outputs.logits\n            \n            # è¨ˆç®— KL divergence\n            # KL(P||Q) = sum(P * log(P/Q))\n            kl_div = F.kl_div(\n                F.log_softmax(student_logits, dim=-1),\n                F.softmax(teacher_logits, dim=-1),\n                reduction='batchmean'\n            )\n            \n            total_kl += kl_div.item()\n            count += 1\n    \n    avg_kl = total_kl / count\n    return avg_kl\n\n# è¨ˆç®— KL Divergence\nkl_divergence = calculate_kl_divergence(\n    teacher_model, \n    student_model, \n    val_dataset, \n    tokenizer,\n    num_samples=50\n)\n\nprint(f\"\\nå­¸ç”Ÿæ¨¡å‹èˆ‡è€å¸«æ¨¡å‹çš„ KL Divergence: {kl_divergence:.4f}\")\nprint(f\"è§£é‡‹: KL Divergence è¶Šå°ï¼Œè¡¨ç¤ºå­¸ç”Ÿæ¨¡å‹çš„è¼¸å‡ºåˆ†å¸ƒè¶Šæ¥è¿‘è€å¸«æ¨¡å‹\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 8: è¦–è¦ºåŒ–çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['å­¸ç”Ÿæ¨¡å‹\\n(è¨“ç·´å‰)', 'å­¸ç”Ÿæ¨¡å‹\\n(è¨“ç·´å¾Œ)', 'è€å¸«æ¨¡å‹']\n",
    "perplexities = [initial_perplexity, final_perplexity, teacher_perplexity]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, perplexities, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.ylabel('Perplexity (è¶Šä½è¶Šå¥½)', fontsize=12, fontweight='bold')\n",
    "plt.title('Knowledge Distillation æ•ˆæœæ¯”è¼ƒ', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, ppl in zip(bars, perplexities):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{ppl:.2f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distillation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nåœ–è¡¨å·²å„²å­˜ç‚º 'distillation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 9: ç”Ÿæˆæ–‡æœ¬æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_generation(teacher, student, tokenizer, prompt, max_length=100):\n    teacher.eval()\n    student.eval()\n    \n    # ä½¿ç”¨ tokenizer() è€Œé encode() ä»¥ç²å¾— attention_mask\n    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    \n    with torch.no_grad():\n        teacher_output = teacher.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=max_length,\n            temperature=0.8,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        teacher_text = tokenizer.decode(teacher_output[0], skip_special_tokens=True)\n        \n        student_output = student.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=max_length,\n            temperature=0.8,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        student_text = tokenizer.decode(student_output[0], skip_special_tokens=True)\n    \n    return teacher_text, student_text\n\ntest_prompts = [\n    \"The future of artificial intelligence\",\n    \"In a distant galaxy\",\n    \"Climate change is\"\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"æ–‡æœ¬ç”Ÿæˆæ¯”è¼ƒ\")\nprint(\"=\"*60)\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\nã€æ¸¬è©¦ {i}ã€‘æç¤ºè©: \\\"{prompt}\\\"\")\n    print(\"-\" * 60)\n    \n    teacher_text, student_text = compare_generation(\n        teacher_model, student_model, tokenizer, prompt, max_length=80\n    )\n    \n    print(f\"\\nğŸ“ è€å¸«æ¨¡å‹:\\n{teacher_text}\")\n    print(f\"\\nğŸ’ å­¸ç”Ÿæ¨¡å‹:\\n{student_text}\")\n    print(\"-\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµå ±å‘Š"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## æ­¥é©Ÿ 10: ç”Ÿæˆå“è³ªè©•ä¼°\n\nè©•ä¼°ç”Ÿæˆæ–‡æœ¬çš„å¤šæ¨£æ€§å’Œè€å¸«/å­¸ç”Ÿæ¨¡å‹è¼¸å‡ºçš„ç›¸ä¼¼åº¦ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_diversity(texts):\n    \"\"\"\n    è¨ˆç®—æ–‡æœ¬é›†åˆçš„è©å½™å¤šæ¨£æ€§\n    \n    Args:\n        texts: æ–‡æœ¬åˆ—è¡¨\n    \n    Returns:\n        unique_token_ratio: å”¯ä¸€ token æ¯”ä¾‹\n        avg_length: å¹³å‡æ–‡æœ¬é•·åº¦\n    \"\"\"\n    all_tokens = []\n    for text in texts:\n        tokens = text.split()\n        all_tokens.extend(tokens)\n    \n    unique_tokens = len(set(all_tokens))\n    total_tokens = len(all_tokens)\n    unique_token_ratio = unique_tokens / total_tokens if total_tokens > 0 else 0\n    avg_length = total_tokens / len(texts) if len(texts) > 0 else 0\n    \n    return unique_token_ratio, avg_length\n\ndef calculate_text_similarity(texts1, texts2):\n    \"\"\"\n    è¨ˆç®—å…©çµ„æ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ Jaccard similarityï¼‰\n    \n    Args:\n        texts1: ç¬¬ä¸€çµ„æ–‡æœ¬åˆ—è¡¨\n        texts2: ç¬¬äºŒçµ„æ–‡æœ¬åˆ—è¡¨\n    \n    Returns:\n        å¹³å‡ Jaccard ç›¸ä¼¼åº¦\n    \"\"\"\n    similarities = []\n    \n    for t1, t2 in zip(texts1, texts2):\n        tokens1 = set(t1.split())\n        tokens2 = set(t2.split())\n        \n        intersection = len(tokens1 & tokens2)\n        union = len(tokens1 | tokens2)\n        \n        similarity = intersection / union if union > 0 else 0\n        similarities.append(similarity)\n    \n    return sum(similarities) / len(similarities) if similarities else 0\n\n# ç”Ÿæˆæ¸¬è©¦æ¨£æœ¬ç”¨æ–¼å“è³ªè©•ä¼°\nprint(\"\\\\nç”Ÿæˆæ¸¬è©¦æ¨£æœ¬ç”¨æ–¼å“è³ªè©•ä¼°...\")\ntest_generation_prompts = [\n    \"Once upon a time\",\n    \"The future of technology\",\n    \"In a world where\",\n    \"Scientists have discovered\",\n    \"Artificial intelligence is\"\n]\n\nteacher_generated = []\nstudent_generated = []\n\nfor prompt in test_generation_prompts:\n    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n    \n    with torch.no_grad():\n        # è€å¸«æ¨¡å‹ç”Ÿæˆ\n        teacher_output = teacher_model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=100,\n            temperature=0.8,\n            top_p=0.95,\n            do_sample=True,\n            num_return_sequences=3,  # æ¯å€‹æç¤ºç”Ÿæˆ 3 å€‹æ¨£æœ¬\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        for output in teacher_output:\n            text = tokenizer.decode(output, skip_special_tokens=True)\n            teacher_generated.append(text)\n        \n        # å­¸ç”Ÿæ¨¡å‹ç”Ÿæˆ\n        student_output = student_model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=100,\n            temperature=0.8,\n            top_p=0.95,\n            do_sample=True,\n            num_return_sequences=3,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        for output in student_output:\n            text = tokenizer.decode(output, skip_special_tokens=True)\n            student_generated.append(text)\n\n# è¨ˆç®—å¤šæ¨£æ€§\nteacher_diversity, teacher_avg_len = calculate_diversity(teacher_generated)\nstudent_diversity, student_avg_len = calculate_diversity(student_generated)\n\nprint(f\"\\\\nğŸ“Š ç”Ÿæˆæ–‡æœ¬å¤šæ¨£æ€§:\")\nprint(f\"  è€å¸«æ¨¡å‹:\")\nprint(f\"    â€¢ å”¯ä¸€ token æ¯”ä¾‹: {teacher_diversity:.4f}\")\nprint(f\"    â€¢ å¹³å‡æ–‡æœ¬é•·åº¦: {teacher_avg_len:.1f} tokens\")\nprint(f\"  å­¸ç”Ÿæ¨¡å‹:\")\nprint(f\"    â€¢ å”¯ä¸€ token æ¯”ä¾‹: {student_diversity:.4f}\")\nprint(f\"    â€¢ å¹³å‡æ–‡æœ¬é•·åº¦: {student_avg_len:.1f} tokens\")\n\n# è¨ˆç®—è€å¸«å’Œå­¸ç”Ÿè¼¸å‡ºçš„ç›¸ä¼¼åº¦\nsimilarity = calculate_text_similarity(teacher_generated, student_generated)\nprint(f\"\\\\nğŸ”— è€å¸«èˆ‡å­¸ç”Ÿè¼¸å‡ºç›¸ä¼¼åº¦:\")\nprint(f\"  â€¢ Jaccard ç›¸ä¼¼åº¦: {similarity:.4f}\")\nprint(f\"  â€¢ è§£é‡‹: ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œè¡¨ç¤ºå­¸ç”Ÿæ¨¡å‹è¶Šèƒ½æ¨¡ä»¿è€å¸«æ¨¡å‹çš„ç”Ÿæˆé¢¨æ ¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## æ­¥é©Ÿ 11: è¦–è¦ºåŒ–è¼¸å‡ºåˆ†å¸ƒæ¯”è¼ƒ\n\næ¯”è¼ƒè€å¸«å’Œå­¸ç”Ÿæ¨¡å‹åœ¨ç›¸åŒè¼¸å…¥ä¸‹çš„ token probability åˆ†å¸ƒã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch.nn.functional as F\n\n# é¸æ“‡ä¸€å€‹æ¸¬è©¦å¥å­ä¾†æ¯”è¼ƒåˆ†å¸ƒ\ntest_sentence = \"The future of artificial intelligence is\"\ninputs = tokenizer(test_sentence, return_tensors='pt').to(device)\n\n# ç²å–è€å¸«å’Œå­¸ç”Ÿçš„è¼¸å‡ºåˆ†å¸ƒ\nwith torch.no_grad():\n    teacher_outputs = teacher_model(**inputs)\n    student_outputs = student_model(**inputs)\n    \n    teacher_probs = F.softmax(teacher_outputs.logits[0, -1, :], dim=-1).cpu().numpy()\n    student_probs = F.softmax(student_outputs.logits[0, -1, :], dim=-1).cpu().numpy()\n\n# ç²å– top-20 æœ€å¯èƒ½çš„ tokens\ntop_k = 20\nteacher_top_indices = teacher_probs.argsort()[-top_k:][::-1]\nstudent_top_indices = student_probs.argsort()[-top_k:][::-1]\n\n# åˆä½µå…©çµ„ top tokens\nall_top_indices = list(set(teacher_top_indices) | set(student_top_indices))\nall_top_indices = sorted(all_top_indices, \n                         key=lambda x: teacher_probs[x] + student_probs[x], \n                         reverse=True)[:top_k]\n\n# æº–å‚™ç¹ªåœ–è³‡æ–™\ntokens = [tokenizer.decode([idx]) for idx in all_top_indices]\nteacher_top_probs = [teacher_probs[idx] for idx in all_top_indices]\nstudent_top_probs = [student_probs[idx] for idx in all_top_indices]\n\n# ç¹ªè£½æ¯”è¼ƒåœ–\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# å·¦åœ–ï¼šä¸¦æ’æ¯”è¼ƒ\nx = np.arange(len(tokens))\nwidth = 0.35\n\nbars1 = ax1.bar(x - width/2, teacher_top_probs, width, label='è€å¸«æ¨¡å‹', \n                color='#45B7D1', alpha=0.8)\nbars2 = ax1.bar(x + width/2, student_top_probs, width, label='å­¸ç”Ÿæ¨¡å‹', \n                color='#4ECDC4', alpha=0.8)\n\nax1.set_xlabel('Token', fontsize=11, fontweight='bold')\nax1.set_ylabel('æ©Ÿç‡', fontsize=11, fontweight='bold')\nax1.set_title(f'Top-{top_k} Token æ©Ÿç‡åˆ†å¸ƒæ¯”è¼ƒ\\\\nè¼¸å…¥: \"{test_sentence}\"', \n              fontsize=12, fontweight='bold', pad=15)\nax1.set_xticks(x)\nax1.set_xticklabels(tokens, rotation=45, ha='right')\nax1.legend(fontsize=10)\nax1.grid(axis='y', alpha=0.3, linestyle='--')\n\n# å³åœ–ï¼šæ•£é»åœ–æ¯”è¼ƒ\nax2.scatter(teacher_top_probs, student_top_probs, alpha=0.6, s=100, color='#FF6B6B')\nax2.plot([0, max(teacher_top_probs)], [0, max(teacher_top_probs)], \n         'k--', alpha=0.3, label='å®Œç¾åŒ¹é…ç·š')\n\nax2.set_xlabel('è€å¸«æ¨¡å‹æ©Ÿç‡', fontsize=11, fontweight='bold')\nax2.set_ylabel('å­¸ç”Ÿæ¨¡å‹æ©Ÿç‡', fontsize=11, fontweight='bold')\nax2.set_title('æ©Ÿç‡åˆ†å¸ƒç›¸é—œæ€§', fontsize=12, fontweight='bold', pad=15)\nax2.legend(fontsize=10)\nax2.grid(alpha=0.3, linestyle='--')\n\n# æ·»åŠ ç›¸é—œä¿‚æ•¸\ncorrelation = np.corrcoef(teacher_top_probs, student_top_probs)[0, 1]\nax2.text(0.05, 0.95, f'ç›¸é—œä¿‚æ•¸: {correlation:.4f}', \n         transform=ax2.transAxes, fontsize=11, \n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.savefig('distribution_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\\\nğŸ“ˆ è¼¸å‡ºåˆ†å¸ƒåˆ†æ:\")\nprint(f\"  â€¢ ç›¸é—œä¿‚æ•¸: {correlation:.4f}\")\nprint(f\"  â€¢ è§£é‡‹: ç›¸é—œä¿‚æ•¸è¶Šæ¥è¿‘ 1ï¼Œè¡¨ç¤ºå­¸ç”Ÿæ¨¡å‹å’Œè€å¸«æ¨¡å‹çš„è¼¸å‡ºåˆ†å¸ƒè¶Šç›¸ä¼¼\")\nprint(f\"\\\\nåœ–è¡¨å·²å„²å­˜ç‚º 'distribution_comparison.png'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"Knowledge Distillation å¯¦é©—ç¸½çµ\")\nprint(\"=\"*70)\n\nprint(f\"\\nğŸ“Š æ¨¡å‹è¦æ ¼:\")\nprint(f\"  â€¢ è€å¸«æ¨¡å‹: GPT-2 ({teacher_params/1e6:.1f}M åƒæ•¸)\")\nprint(f\"  â€¢ å­¸ç”Ÿæ¨¡å‹: GPT-2-Small ({student_params/1e6:.1f}M åƒæ•¸)\")\nprint(f\"  â€¢ åƒæ•¸å£“ç¸®æ¯”: {teacher_params/student_params:.2f}x\")\n\nprint(f\"\\nğŸ“š è¨“ç·´è³‡æ–™:\")\nprint(f\"  â€¢ è¨“ç·´æ¨£æœ¬æ•¸: {len(train_texts)}\")\nprint(f\"  â€¢ é©—è­‰æ¨£æœ¬æ•¸: {len(val_texts)}\")\nprint(f\"  â€¢ è³‡æ–™ä¾†æº: è€å¸«æ¨¡å‹ç”Ÿæˆ\")\n\nprint(f\"\\nğŸ“ˆ è©•ä¼°çµæœ:\")\nprint(f\"\\n  1ï¸âƒ£ Perplexityï¼ˆé æ¸¬æº–ç¢ºåº¦ï¼‰:\")\nprint(f\"     â€¢ è€å¸«æ¨¡å‹: {teacher_perplexity:.2f}\")\nprint(f\"     â€¢ å­¸ç”Ÿæ¨¡å‹ (è¨“ç·´å‰): {initial_perplexity:.2f}\")\nprint(f\"     â€¢ å­¸ç”Ÿæ¨¡å‹ (è¨“ç·´å¾Œ): {final_perplexity:.2f}\")\nprint(f\"     â€¢ æ”¹å–„ç‡: {improvement:.2f}%\")\n\nprint(f\"\\n  2ï¸âƒ£ KL Divergenceï¼ˆåˆ†å¸ƒç›¸ä¼¼åº¦ï¼‰:\")\nprint(f\"     â€¢ KL Divergence: {kl_divergence:.4f}\")\nprint(f\"     â€¢ è§£é‡‹: å€¼è¶Šå°è¡¨ç¤ºå­¸ç”Ÿè¶Šæ¥è¿‘è€å¸«çš„è¼¸å‡ºåˆ†å¸ƒ\")\n\nprint(f\"\\n  3ï¸âƒ£ ç”Ÿæˆå“è³ªï¼ˆå¤šæ¨£æ€§ï¼‰:\")\nprint(f\"     â€¢ è€å¸«æ¨¡å‹å”¯ä¸€ token æ¯”ä¾‹: {teacher_diversity:.4f}\")\nprint(f\"     â€¢ å­¸ç”Ÿæ¨¡å‹å”¯ä¸€ token æ¯”ä¾‹: {student_diversity:.4f}\")\nprint(f\"     â€¢ è€å¸«æ¨¡å‹å¹³å‡é•·åº¦: {teacher_avg_len:.1f} tokens\")\nprint(f\"     â€¢ å­¸ç”Ÿæ¨¡å‹å¹³å‡é•·åº¦: {student_avg_len:.1f} tokens\")\n\nprint(f\"\\n  4ï¸âƒ£ è¼¸å‡ºç›¸ä¼¼åº¦:\")\nprint(f\"     â€¢ Jaccard ç›¸ä¼¼åº¦: {similarity:.4f}\")\nprint(f\"     â€¢ æ©Ÿç‡åˆ†å¸ƒç›¸é—œä¿‚æ•¸: {correlation:.4f}\")\n\nprint(f\"\\nğŸ¯ çŸ¥è­˜è’¸é¤¾æ•ˆæœåˆ†æ:\")\nachievement_rate = (teacher_perplexity / final_perplexity) * 100\nprint(f\"  â€¢ å­¸ç”Ÿæ¨¡å‹é”æˆç‡: {achievement_rate:.2f}%\")\nprint(f\"    (ç›¸å°æ–¼è€å¸«æ¨¡å‹çš„ perplexity è¡¨ç¾)\")\n\nefficiency_gain = (teacher_params / student_params) / (final_perplexity / teacher_perplexity)\nprint(f\"  â€¢ æ•ˆç‡æå‡: {efficiency_gain:.2f}x\")\nprint(f\"    (åƒæ•¸æ¸›å°‘ vs. æ•ˆèƒ½æå¤±æ¯”)\")\n\n# ç¶œåˆè©•ä¼°\nprint(f\"\\nâœ… ç¶œåˆçµè«–:\")\nprint(f\"  âœ“ å­¸ç”Ÿæ¨¡å‹æˆåŠŸå¾è€å¸«æ¨¡å‹ç”Ÿæˆçš„è³‡æ–™ä¸­å­¸ç¿’!\")\nprint(f\"  âœ“ ä½¿ç”¨åƒ… {student_params/teacher_params*100:.1f}% çš„åƒæ•¸é‡\")\nprint(f\"  âœ“ Perplexity é”åˆ°è€å¸«æ¨¡å‹ {achievement_rate:.1f}% çš„æ•ˆèƒ½\")\nprint(f\"  âœ“ è¼¸å‡ºåˆ†å¸ƒèˆ‡è€å¸«æ¨¡å‹ç›¸é—œä¿‚æ•¸: {correlation:.3f}\")\nprint(f\"  âœ“ æˆåŠŸå¯¦ç¾æ¨¡å‹å£“ç¸®ï¼ŒåŒæ™‚ä¿æŒåˆç†çš„ç”Ÿæˆå“è³ª\")\n\nprint(f\"\\nğŸ’¡ è©•ä¼°æ–¹æ³•ç¸½çµ:\")\nprint(f\"  æœ¬å¯¦é©—ä½¿ç”¨äº†å¤šç¨®è©•ä¼°æŒ‡æ¨™ä¾†å…¨é¢è¡¡é‡çŸ¥è­˜è’¸é¤¾æ•ˆæœ:\")\nprint(f\"  â€¢ Perplexity: è¡¡é‡é æ¸¬æº–ç¢ºåº¦\")\nprint(f\"  â€¢ KL Divergence: è¡¡é‡åˆ†å¸ƒç›¸ä¼¼åº¦\")\nprint(f\"  â€¢ å¤šæ¨£æ€§åˆ†æ: è¡¡é‡ç”Ÿæˆæ–‡æœ¬å“è³ª\")\nprint(f\"  â€¢ ç›¸ä¼¼åº¦æŒ‡æ¨™: è¡¡é‡è€å¸«/å­¸ç”Ÿè¼¸å‡ºä¸€è‡´æ€§\")\nprint(f\"  â€¢ è¦–è¦ºåŒ–æ¯”è¼ƒ: ç›´è§€å±•ç¤ºæ¨¡å‹å·®ç•°\")\n\nprint(\"\\n\" + \"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}